#' This function fits separate classication models for each response variable in a dataset. Y (response variables) should be binary (0/1). Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imblanace can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#' @example
#' model1 <-
#' rand_forest(trees = 100, mode = "classification") %>% #this should cope with multinomial data alreadf
#'   set_engine("ranger", importance = c("impurity","impurity_corrected")) %>% #model is not tuned to increase computational speed
#'  set_mode("classification")
#'
#' yhats <- mrIMLpredicts(X= venviro_variables,Y=response data, model1=model1, balance_data='no', model='classification', parallel = TRUE,
#'tune_grid_size=5, seed = sample.int(1e8, 1)))
#'@export
mrIMLpredicts<- function(X, Y, model1, balance_data ='no', model='regression', parallel = TRUE, transformY='log', tune_grid_size= 10, seed = sample.int(1e8, 1) ) {
if(parallel==TRUE){
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
}
n_response<- length(X)
# Run model 1 for each parasite; a simple logistic regression with a single covariate
# in this case but note that model 1 can be any model of the user's choice,
# from simple regressions to complex hierarchical or deep learning models.
# Different structures can also be used for each species to handle mixed multivariate outcomes
mod1_perf <- NULL #place to save performance matrix
#yhats <- for(i in 1:length(X)) {
#rhats <- lapply(seq(1, n_variables), function(species){
internal_fit_function <- function( i ){
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
set.seed(seed)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
if (model=='classification'){
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
}
if (model=='regression'){
# select the best model
best_m <- tune_m %>%
select_best("rmse")
}
# Calculate probability predictions for the fitted training data.
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
mod1_k <- final_model %>%
fit(data = data_train)
# Calculate deviance residuals
if (model=='classification'){
#predictions
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid= NULL
}
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
}
#fit on the training set and evaluate on test set.
yhats <- lapply(seq(1,n_response), internal_fit_function)
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
}
}
yhats <- mrIMLpredicts(X= X, Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5, seed = sample.int(1e8, 1))
yhats
#'Wrapper to generate multi-response predictive models.
#'@param Y A \code{dataframe} is a response variable data set (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param balance_data A \code{character} 'up', 'down' or 'no'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'@param tune_grid_size A \code{numeric} sets the grid size for hyperparamter tuning. Larger grid sizes increase computational time.
#'@details This function produces yhats that used in all model characteristics for subsequent functions.
#' This function fits separate classication models for each response variable in a dataset. Y (response variables) should be binary (0/1). Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imblanace can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#' @example
#' model1 <-
#' rand_forest(trees = 100, mode = "classification") %>% #this should cope with multinomial data alreadf
#'   set_engine("ranger", importance = c("impurity","impurity_corrected")) %>% #model is not tuned to increase computational speed
#'  set_mode("classification")
#'
#' yhats <- mrIMLpredicts(X= venviro_variables,Y=response data, model1=model1, balance_data='no', model='classification', parallel = TRUE,
#'tune_grid_size=5, seed = sample.int(1e8, 1)))
#'@export
mrIMLpredicts<- function(X, Y, model1, balance_data ='no', model='regression', parallel = TRUE, transformY='log', tune_grid_size= 10, seed = sample.int(1e8, 1) ) {
if(parallel==TRUE){
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
}
n_response<- length(X)
# Run model 1 for each parasite; a simple logistic regression with a single covariate
# in this case but note that model 1 can be any model of the user's choice,
# from simple regressions to complex hierarchical or deep learning models.
# Different structures can also be used for each species to handle mixed multivariate outcomes
mod1_perf <- NULL #place to save performance matrix
#yhats <- for(i in 1:length(X)) {
#rhats <- lapply(seq(1, n_variables), function(species){
internal_fit_function <- function( i ){
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
set.seed(seed)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
if (model=='classification'){
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
}
if (model=='regression'){
# select the best model
best_m <- tune_m %>%
select_best("rmse")
}
# Calculate probability predictions for the fitted training data.
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
mod1_k <- final_model %>%
fit(data = data_train)
# Calculate deviance residuals
if (model=='classification'){
#predictions
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid= NULL
}
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#save data
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
}
#fit on the training set and evaluate on test set.
yhats <- lapply(seq(1,n_response), internal_fit_function)
}
yhats <- mrIMLpredicts(X= X, Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5, seed = sample.int(1e8, 1))
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf[[1]]
#'Wrapper to calculate performance metrics (Mathews correlation coefficent, sensitivity and specificity) for each model for each response variable.
#'@param yhats A \code{list} is the list generated by mrIMLpredicts
#'@param model1 A \code{list}  #the model used to generate the yhats object
#'@param X  A \code{dataframe} #is a response variable data set (specie, SNPs etc).
#'
#'@example
#' \dontrun{
#' ModelPerf <- mrIMLperformance(yhats, model1, X=X) }
#'
#'@details Outputs a dataframe of commonly used metric that can be used to compare model performance of classification models. Performance metrics are based on testing data. But MCC is useful (higher numbers = better fit)
#'
#'@export
mrIMLperformance <- function(yhats, model1, X, model='regression'){
n_response<- length(yhats)
mod_perf <- NULL
#  yList <- yhats %>% purrr::map(pluck('yhatT')) #get the training yhats all together
bList <- yhats %>% purrr::map(pluck('last_mod_fit')) ## fix the model ID
if (model=='classification'){
for( i in 1:n_response) {
#get already calculated ROC and accuracy
met1 <- as.data.frame(bList[[i]]$.metrics) ####
roc <- met1$.estimate[2]
#accuracy <-  met1$.estimate[1]
#data from best model to get MCC, specificity and sensivity using yardstick
yd <- as.data.frame(bList[[i]]$.predictions) ###
mathews <-  yardstick::mcc(yd,class, .pred_class) #yardstick is the tidymodels performance package.
mathews <- mathews$.estimate #extract mcc
sen <- yardstick::sens(yd,class, .pred_class)
sen <- sen$.estimate
spe <- yardstick::spec(yd,class, .pred_class)
spe <- spe$.estimate
#add some identifiers
mod_name <- class(model1)[1] #extracts model name
#model1_perf <- c(mod_name, mathews, sen, spe)
sp <- names(X[i])
prev <- sum(X[i])/nrow(X)
#save all the metrics
mod_perf[[i]] <- c( sp, mod_name, roc, mathews, sen, spe, prev)
}
mod1_perf<- do.call(rbind, mod_perf)
mod1_perf <- as.data.frame( mod1_perf)
colnames(mod1_perf) <- c('response', 'model_name', 'roc_AUC', 'mcc', 'sensitivity', 'specificity', 'prevalence')
Global_summary<- as.numeric(as.character(unlist(mod1_perf$roc_AUC))) #cant be mcc as it goes negative
Global_summary[is.na(Global_summary)] <- 0
Global_summary <-  mean(Global_summary)
}
if (model=='regression'){
for( i in 1:n_response) {
met1 <- as.data.frame(bList[[i]]$.metrics) ####
rmse <- met1$.estimate[1]
rsq  <- met1$.estimate[2]
#add some identifiers
mod_name <- class(model1)[1] #extracts model name
sp <- names(X[i])
#save all the metrics
# mod_perf[[i]] <- c( sp, mod_name, rmse, rsq)
mod_perf[[i]] <- data.frame( sp, mod_name, rmse, rsq)
}
mod1_perf<- do.call(rbind, mod_perf)
mod1_perf <- as.data.frame( mod1_perf)
colnames(mod1_perf) <- c('response', 'model_name', 'rmse', 'rsquared')
Global_summary<- as.numeric(as.character(unlist(mod1_perf$rmse)))
#Global_summary[is.na(Global_summary)] <- 0
#Global_summary <-  mean(Global_summary)
Global_summary <-  mean(Global_summary, na.rm = TRUE)
}
#calculate mean for all response variables
#Global_summary <- mean(as.numeric(as.character(unlist( mod1_perf[[5]]))), na.rm = FALSE) #mean testing mcc
#Global_summary <- mean(as.numeric(as.character(unlist( ModelPerf[[1]]$mcc)))) #mean testing
#
return (list(mod1_perf, Global_summary))
}
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf[[1]]
devtools:: document()
rm(list = c("mrIMLperformance", "mrIMLpredicts"))`
)
<<<<<<< HEAD
'
''
""
`
rm(list = c("mrIMLperformance", "mrIMLpredicts"))
devtools:: document()
devtools:: document()
=======
library(LEA)
?gfData
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis); library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel)
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("LEA")
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis); library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel)
#library(LEA)
# read in data file with minor allele freqs & env/space variables
gfData; str(gfData)
envGF <- gfData[,3:13] # get climate & MEM variables
Y <- envGF #for simplicity
# build individual SNP datasets
SNPs_ref <- gfData[,grep("REFERENCE",colnames(gfData))] # reference
GI5 <- gfData[,grep("GI5",colnames(gfData))] # GIGANTEA-5 (GI5)
X <- GI5 #for this example we are going to focus on the adaptive SNPs in the GI5 region.
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
gfData; str(gfData)
str()
str
?str\
?str
pkgdown::build_site()
build_home()
build_news()
pkgdown::build_site()
pkgdown::build_site()
library(pkgdown)
pkgdown::build_site()
build_news()
#init_site()
sink()
#init_site()
sink()
build_news()
pkgdown::build_site()
library(pkgdown)
#init_site()
sink()
pkgdown::build_site()
library(pkgdown)
pkgdown::build_site()
usethis::use_github_action("pkgdown")
usethis::use_github_action("pkgdown")
pkgdown::build_site()
getwd()
build_news()
library(pkgdown)
build_home()
pkgdown::build_site()
library(pkgdown)
library(pkgdown)
pkgdown::build_site()
<<<<<<< HEAD
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
library(mrIML)
remove.packages("mrIML", lib="~/R/win-library/3.6")
devtools::install_github('nfj1380/mrIML')
install.packages("xgboost")
pkgdown::build_site()
library(mrIML)
library(mrIML)
model1 <-
rand_forest(trees = 100, mode = "classification") %>% #100 trees are set for brevity
set_engine("ranger", importance = c("impurity","impurity_corrected")) %>%# select the engine/package that underlies the model
set_mode("classification")# choose either the continuous "regression" or binary "classification" mode
library(mrIML)
#other package needed:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(mrIML)
#other package needed:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost);library(flashlight);
library(ggrepel); library(parsnip);library(rsample); library(workflows)
VI <- mrVip(yhats, Y=Y)
detach("package:mrIML", unload = TRUE)
library(mrIML)
VI <- mrVip(yhats, Y=Y)
# Define set of features
FeaturesnoNA<-Features[complete.cases(Features), ]
Y <- FeaturesnoNA #for simplicity
# Define set the outcomes of interst
fData <- filterRareCommon (Responsedata, lower=0.4, higher=0.7)
X <- fData #
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', mod='classification', parallel = TRUE)
model1 <-
rand_forest(trees = 100, mode = "classification") %>% #100 trees are set for brevity
set_engine("ranger", importance = c("impurity","impurity_corrected")) %>%# select the engine/package that underlies the model
set_mode("classification")# choose either the continuous "regression" or binary "classification" mode
data <- gfData[1:20]
head(data)
# Define set of features
FeaturesnoNA<-Features[complete.cases(Features), ]
Y <- FeaturesnoNA #for simplicity
# Define set the outcomes of interst
fData <- filterRareCommon (Responsedata, lower=0.4, higher=0.7)
X <- fData #
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', mod='classification', parallel = TRUE)
#save(yhats, file='logreg_model')
ModelPerf <- mrIMLperformance(yhats, model1, X=X) #
ModelPerf[[2]]
VI <- mrVip(yhats, Y=Y)
plot_vi(VI=VI,  X=X,Y=Y, modelPerf=ModelPerf, cutoff= 0, plot.pca='yes') #the cutoff reduces the number of individual models printed in the second plot.
pkgdown::build_site()
20216+4198
25000-24414
13*30
30+36+4+4
74*13
20216+4198+586
100*40
4000*4
16000*12
20*100
2000*4
devtools::session_info()
360/5
25+45+80
25+45+80+50+50
25+45+80+50+80
35+45+80+50+80
35+75+80+50+80
35+75+80+60+80
35+75+80+70+80
45+75+80+70+80
45+75+85+70+80
45+75+85+75+80
30/5
30/54
30/4
30/2
30/3
30/4
6*5
=======
>>>>>>> dbccd43166391d87671ae0def0ce3a4cc8e88027
<<<<<<< HEAD
library(pkgdown)
pkgdown::build_site()
build_news()
build_home()
pkgdown::build_site()
=======
>>>>>>> abce6e7093966e7d10b493afb89473eb098516d9
>>>>>>> 03c98ad2a032d73ac4f16dd64c964759fa57d22b

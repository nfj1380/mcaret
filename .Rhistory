)
# Create explainer
if (response == "single") {
mfl <- flashlight(
model = yhats[[index]]$mod1_k,
label = colnames(Y)[index],
data = cbind(Y[index], X),
y = colnames(Y)[index],
predict_function = pred_fun,
metrics = metrics
)
} else if (response == "multi") {
models <- lapply(yhats, `[[`, "mod1_k")
fl_list <- vector("list", length(yhats))
for (i in seq_along(fl_list)) {
fl_list[[i]] <- flashlight(
model = yhats[[i]]$mod1_k,
label = colnames(Y)[i],
y = colnames(Y)[i]
)
}
mfl <- multiflashlight(
fl_list,
data = cbind(Y, X),
predict_function = pred_fun,
metrics = metrics
)
}
} else {
stop("Invalid mode selected. Please choose either 'classification' or 'regression'.")
}
return(mfl)
}
#source('~/MrIML/mrIML/R/mrFlashlight.R')
flashlightObj <- mrFlashlight(yhats=yhats_rf, X=X, Y=Y, response = "multi", mode="regression")
profileData_pd <- light_profile(flashlightObj, v = "bio_1") #partial dependencies
mrProfileplot(profileData_pd , sdthresh =0.01)
document()
devtools::document()
rm(list = c("mrFlashlight"))
devtools::document()
devtools::document()
devtools::document()
devtools::document()
remove.packages("mrIML")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
devtools:: install_github('nfj1380/mrIML')
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis); library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel); library(iml); library(plyr);
library(future.apply)
#library(LEA)
#if (!requireNamespace("BiocManager", quietly = TRUE))
# install.packages("BiocManager") #LEA requires Biocmanager
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
X <- FeaturesnoNA #for simplicity
#for more efficent testing for interactions (more variables more interacting pairs)
X <- FeaturesnoNA[c(1:3)] #three features only
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
pacman:: p_load(
vip, tidymodels, randomForest, caret, gbm,
tidyverse, parallel, doParallel, themis, viridis,
janitor, hrbrthemes, xgboost, vegan, flashlight,
ggrepel, iml, plyr, future.apply
)
#library(LEA)
#if (!requireNamespace("BiocManager", quietly = TRUE))
# install.packages("BiocManager") #LEA requires Biocmanager
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
X <- FeaturesnoNA #for simplicity
#for more efficent testing for interactions (more variables more interacting pairs)
X <- FeaturesnoNA[c(1:3)] #three features only
#Optional: Filter rare/common SNPs or species. Retaining minor allelle frequncies >0.1 and removing common allelles (occur>0.9)
fData <- filterRareCommon (Responsedata, lower=0.4, higher=0.7)
Y <- fData #for simplicity when comparing
#another option at this stage is to filter response that are strongly correlated with each other.
#df2 <- cor(X) #find correlations
#hc <-  findCorrelation(df2, cutoff=0.5) # put any value as a "cutoff".
#hc <-  sort(hc)
#X <-  X[,-c(hc)] #
model_rf <-
rand_forest(trees = 100, mode = "classification", mtry = tune(), min_n = tune()) %>% #100 trees are set for brevity. Aim to start with 1000
set_engine("randomForest")
#we are tuning mtry and min_n
#set up parallel processing. If you don't add the code below it will still work but just on one core.
## detectCores() #check how many cores you have available. We suggest keeping one core free for internet browsing etc.
##cl <- parallel::makeCluster(4)
##plan(cluster, workers=cl)
View(Y)
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',  tune_grid_size=5,
seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',v=2,  tune_grid_size=5,
seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',k=2,  tune_grid_size=5,
seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',k=3,  tune_grid_size=5,
seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',k=5,  tune_grid_size=5,
seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',k=5,  tune_grid_size=5,
seed = sample.int(1e8, 1), racing=F ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
model_rf <-
rand_forest(trees = 100, mode = "classification", mtry = tune(), min_n = tune()) %>% #100 trees are set for brevity. Aim to start with 1000
set_engine("randomForest")
#we are tuning mtry and min_n
#set up parallel processing. If you don't add the code below it will still work but just on one core.
## detectCores() #check how many cores you have available. We suggest keeping one core free for internet browsing etc.
cl <- parallel::makeCluster(4)
plan(cluster, workers=cl)
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',k=5,  tune_grid_size=5,
seed = sample.int(1e8, 1), racing=F ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
ModelPerf_rf <- mrIMLperformance(yhats_rf, Model=model_rf, Y=Y, mode='classification')
ModelPerf_rf[[1]] #predictive performance for individual responses
ModelPerf_rf[[2]]#overall predictive performance
ModelPerf_rf
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
pacman:: p_load(
vip, tidymodels, randomForest, caret, gbm,
tidyverse, parallel, doParallel, themis, viridis,
janitor, hrbrthemes, xgboost, vegan, flashlight,
ggrepel, iml, plyr, future.apply
)
#library(LEA)
#if (!requireNamespace("BiocManager", quietly = TRUE))
# install.packages("BiocManager") #LEA requires Biocmanager
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
X <- FeaturesnoNA #for simplicity
#for more efficent testing for interactions (more variables more interacting pairs)
X <- FeaturesnoNA[c(1:3)] #three features only
#Optional: Filter rare/common SNPs or species. Retaining minor allelle frequncies >0.1 and removing common allelles (occur>0.9)
fData <- filterRareCommon (Responsedata, lower=0.4, higher=0.7)
Y <- fData #for simplicity when comparing
#another option at this stage is to filter response that are strongly correlated with each other.
#df2 <- cor(X) #find correlations
#hc <-  findCorrelation(df2, cutoff=0.5) # put any value as a "cutoff".
#hc <-  sort(hc)
#X <-  X[,-c(hc)] #
model_rf <-
rand_forest(trees = 100, mode = "classification", mtry = tune(), min_n = tune()) %>% #100 trees are set for brevity. Aim to start with 1000
set_engine("randomForest")
#we are tuning mtry and min_n
#set up parallel processing. If you don't add the code below it will still work but just on one core.
## detectCores() #check how many cores you have available. We suggest keeping one core free for internet browsing etc.
cl <- parallel::makeCluster(4)
plan(cluster, workers=cl)
yhats_rf <- mrIMLpredicts(X=X,Y=Y, Model=model_rf, balance_data='no', mode='classification',k=5,  tune_grid_size=5,
seed = sample.int(1e8, 1), racing=F ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
ModelPerf_rf <- mrIMLperformance(yhats_rf, Model=model_rf, Y=Y, mode='classification')
ModelPerf_rf[[1]] #predictive performance for individual responses
ModelPerf_rf[[2]]#overall predictive performance
ModelPerf_rf
model_glm <- #model used to generate yhat
logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
yhats_glm <- mrIMLpredicts(X=X,Y=Y, Model=model_glm, balance_data='no', mode='classification',  tune_grid_size=5, seed = sample.int(1e8, 1) )
model_glm <- #model used to generate yhat
logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
yhats_glm <- mrIMLpredicts(X=X,Y=Y, Model=model_glm, balance_data='no', mode='classification',  tune_grid_size=5, seed = sample.int(1e8, 1), racing=F )
model_glm <- #model used to generate yhat
logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
yhats_glm <- mrIMLpredicts(X=X,Y=Y, Model=model_glm, balance_data='no', mode='classification', k=5,  tune_grid_size=5, seed = sample.int(1e8, 1), racing=F )
model_glm <- #model used to generate yhat
logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
yhats_glm <- mrIMLpredicts(X=X,Y=Y, Model=model_glm, balance_data='no', mode='classification', k=5,  tune_grid_size=5, seed = sample.int(1e8, 1), racing=F )
#save(yhats, file='logreg_model')
ModelPerf_glm <- mrIMLperformance(yhats_glm, Model=model_glm, Y=Y, mode='classification') #
ModelPerf_glm[[2]]
model_glm <- #model used to generate yhat
logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
yhats_glm <- mrIMLpredicts(X=X,Y=Y, Model=model_glm, balance_data='no', mode='classification', k=5,  tune_grid_size=5, seed = sample.int(1e8, 1), racing=F )
#save(yhats, file='logreg_model')
ModelPerf_glm <- mrIMLperformance(yhats_glm, Model=model_glm, Y=Y, mode='classification') #
ModelPerf_glm[[2]]
model_glm <- #model used to generate yhat
logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
yhats_glm <- mrIMLpredicts(X=X,Y=Y, Model=model_glm, balance_data='no', mode='classification', k=5,  tune_grid_size=5, seed = sample.int(1e8, 1), racing=F )
#save(yhats, file='logreg_model')
ModelPerf_glm <- mrIMLperformance(yhats_glm, Model=model_glm, Y=Y, mode='classification') #
ModelPerf_glm[[2]]
plots <- mrPerformancePlot(ModelPerf1 =ModelPerf_glm, ModelPerf2 = ModelPerf_rf, mod_names=c('linear_reg','rand_forest'), mode='regression' )
model_glm <- #model used to generate yhat
logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
yhats_glm <- mrIMLpredicts(X=X,Y=Y, Model=model_glm, balance_data='no', mode='classification', k=5,  tune_grid_size=5, seed = sample.int(1e8, 1), racing=F )
#save(yhats, file='logreg_model')
ModelPerf_glm <- mrIMLperformance(yhats_glm, Model=model_glm, Y=Y, mode='classification') #
ModelPerf_glm[[2]]
plots <- mrPerformancePlot(ModelPerf1 =ModelPerf_glm, ModelPerf2 = ModelPerf_rf, mod_names=c('linear_reg','rand_forest'), mode='classification' )
VI <- mrVip(yhats_rf, X=X)
bs_fiv <- mrBootstrap(yhats=yhats_rf,Y=Y, #n
num_bootstrap = 10, downsample = FALSE, mode='classification')
#' Bootstrap model predictions
#'
#' This function bootstraps model predictions and generates variable profiles
#' for each response variable based on the provided yhats.
#'
#' @param yhats A list of model predictions mrIMLpredicts
#' @param num_bootstrap The number of bootstrap samples to generate (default: 10).
#' @param Y The response data (default: Y).
#'@param mode \code{character}'classification' or 'regression' i.e., is the generative model a regression or classification?
#' @param downsample Do the bootstrap samples need to be downsampled? Default is FALSE
#' @return A list containing bootstrap samples of variable profiles for each response variable.
#' @export
#' @examples
#' \dontrun{
#' # Example usage:
#' #set up analysis
#' Y <- dplyr::select(Bird.parasites, -scale.prop.zos)%>%
#' dplyr::select(sort(names(.)))#response variables eg. SNPs, pathogens, species....
#' X <- dplyr::select(Bird.parasites, scale.prop.zos) # feature set
#' X1 <- Y %>%
#' dplyr::select(sort(names(.)))
#'model_rf <-
#' rand_forest(trees = 100, mode = "classification", mtry = tune(), min_n = tune()) %>% #100 trees are set for brevity. Aim to start with 1000
#' set_engine("randomForest")
#' yhats_rf <- mrIMLpredicts(X=X, Y=Y,
#'X1=X1,'Model=model_rf ,
#'balance_data='no',mode='classification',
#'tune_grid_size=5,seed = sample.int(1e8, 1),'morans=F,
#'prop=0.7, k=5, racing=T) #
#'
#'bs_analysis <- mrBootstrap(yhats=yhats_rf,Y=Y, num_bootstrap = 50, mode='classification')
#'}
mrBootstrap <- function(yhats, num_bootstrap = 10, Y=Y, downsample=FALSE, mode='classification') {
n_response <- length(yhats)
pb <- txtProgressBar(min = 0, max = n_response, style = 3)
# pb <- progress::progress_bar$new(format = "[:bar] :percent ETA: :eta", total = n_response )
internal_fit_function <- function(k) {
setTxtProgressBar(pb, k) #progressbar marker
# pb$tick()
features <- colnames(yhats[[k]]$data)[-1]
n <- nrow(yhats[[k]]$data)
pd_raw <- vector("list", num_bootstrap)  # Initialize pd_raw as a list
for (i in 1:num_bootstrap) {
# Initialize the bootstrap sample
bootstrap_sample <- NULL
if (downsample==TRUE) {
# Determine the number of samples to draw for each class
class_counts <- table(Y[[k]])
min_class_count <- min(class_counts)
sample_size <- min_class_count * length(class_counts)
# Sample from each class to balance classes
for (cls in unique(Y[[k]])) {
cls_indices <- sample(which(Y[[k]] == cls), size = min_class_count, replace = FALSE)
bootstrap_sample <- rbind(bootstrap_sample, yhats[[k]]$data[cls_indices, ])
}
} else {
# Convert the data frame to a data table
data_table <- data.table::as.data.table(yhats[[k]]$data)
# Generate random row indices
sample_indices <- sample(1:n, replace = TRUE)
# Create the bootstrap sample using data table syntax
bootstrap_sample <- data_table[sample_indices]
}
# Extract the workflow from the best fit
wflow <- yhats[[k]]$last_mod_fit %>% tune::extract_workflow()
# Add the bootstrap data to the workflow
wflow$data <- bootstrap_sample
# Fit the model using the bootstrap sample
model_fit <- fit(wflow, data = bootstrap_sample)
# Create explainer
var_names <- names(yhats[[k]]$data)[-1]
if(mode=='classification'){
#metric list for flashlight.
metrics <- list(
logloss = MetricsWeighted::logLoss,
`ROC AUC` = MetricsWeighted::AUC,
`% Dev Red` = MetricsWeighted::r_squared_bernoulli
)
pred_fun <- function(m, dat) {
predict(
m, dat[, colnames(bootstrap_sample)[-1], drop = FALSE],
type = "prob"
)$`.pred_1`
}
} else {
pred_fun <- function(m, dat) {
predict(m, dat[, colnames(X), drop = FALSE])[[".pred"]]
}
# List of metrics
metrics = list(
rmse = MetricsWeighted::rmse,
`R-squared` = MetricsWeighted::r_squared
)
}
fl <- flashlight(
model = model_fit,
label = 'class',
data = bootstrap_sample,
y = 'class',
predict_function = pred_fun,
metrics = metrics
)
for (j in seq_along(var_names)) {
pd_ <- light_profile(fl, v = paste0(var_names[j]))
#add number of boostrap.
bs_rep <- data.frame( bootstrap=rep(i, nrow(pd_$data)))
#response name
bs_name <- data.frame( response=rep(names(Y[k]), nrow(pd_$data)))
pd_data <-data.frame(cbind(pd_$data),  bs_name, bs_rep) #add bootstrap
pd_raw[[i]][[var_names[j]]] <- pd_data  # Save pd_ as a list element
}
}
gc() #clear junk
return(pd_raw)  # Return pd_raw as a list
}
bstraps_pd_list <- future_lapply(seq(1, n_response), internal_fit_function, future.seed = TRUE)
return(bstraps_pd_list)
}
bs_fiv <- mrBootstrap(yhats=yhats_rf,Y=Y, #n
num_bootstrap = 10, downsample = FALSE, mode='classification')
bs_impVI <- mrvip(
mrBootstrap_obj = bs_fiv ,
yhats = yhats_rf,
X = X,
Y = Y,
mode = 'classification',
threshold = 0.0,
global_top_var = 10,
local_top_var = 5,
taxa = NULL,
ModelPerf = ModelPerf_rf_downSamp
)
bs_fiv <- mrBootstrap(yhats=yhats_rf,Y=Y, #n
num_bootstrap = 10, downsample = FALSE, mode='classification')
bs_impVI <- mrvip(
mrBootstrap_obj = bs_fiv ,
yhats = yhats_rf,
X = X,
Y = Y,
mode = 'classification',
threshold = 0.0,
global_top_var = 10,
local_top_var = 5,
taxa = NULL,
ModelPerf = ModelPerf_rf
)
??arrangeGrob
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
pacman:: p_load(
vip, tidymodels, randomForest, caret, gbm,
tidyverse, parallel, doParallel, themis, viridis,
janitor, hrbrthemes, xgboost, vegan, flashlight,
ggrepel, iml, plyr, future.apply, gridExtra
)
#library(LEA)
#if (!requireNamespace("BiocManager", quietly = TRUE))
# install.packages("BiocManager") #LEA requires Biocmanager
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
X <- FeaturesnoNA #for simplicity
#for more efficent testing for interactions (more variables more interacting pairs)
X <- FeaturesnoNA[c(1:3)] #three features only
bs_fiv <- mrBootstrap(yhats=yhats_rf,Y=Y, #n
num_bootstrap = 10, downsample = FALSE, mode='classification')
bs_impVI <- mrvip(
mrBootstrap_obj = bs_fiv ,
yhats = yhats_rf,
X = X,
Y = Y,
mode = 'classification',
threshold = 0.0,
global_top_var = 10,
local_top_var = 5,
taxa = NULL,
ModelPerf = ModelPerf_rf
)
??plot_grid
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
pacman:: p_load(
vip, tidymodels, randomForest, caret, gbm,
tidyverse, parallel, doParallel, themis, viridis,
janitor, hrbrthemes, xgboost, vegan, flashlight,
ggrepel, iml, plyr, future.apply, gridExtra, cowplot
)
#library(LEA)
#if (!requireNamespace("BiocManager", quietly = TRUE))
# install.packages("BiocManager") #LEA requires Biocmanager
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
X <- FeaturesnoNA #for simplicity
#for more efficent testing for interactions (more variables more interacting pairs)
X <- FeaturesnoNA[c(1:3)] #three features only
bs_fiv <- mrBootstrap(yhats=yhats_rf,Y=Y, #n
num_bootstrap = 10, downsample = FALSE, mode='classification')
bs_impVI <- mrvip(
mrBootstrap_obj = bs_fiv ,
yhats = yhats_rf,
X = X,
Y = Y,
mode = 'classification',
threshold = 0.0,
global_top_var = 10,
local_top_var = 5,
taxa = NULL,
ModelPerf = ModelPerf_rf
)
str(Y)
bs_fiv <- mrBootstrap(yhats=yhats_rf,Y=Y, #n
num_bootstrap = 10, downsample = FALSE, mode='classification')
bs_impVI <- mrvip(
mrBootstrap_obj = bs_fiv ,
yhats = yhats_rf,
X = X,
Y = Y,
mode = 'classification',
threshold = 0.0,
global_top_var = 10,
local_top_var = 5,
taxa = pol_132,
ModelPerf = ModelPerf_rf
)
bs_fiv <- mrBootstrap(yhats=yhats_rf,Y=Y, #n
num_bootstrap = 10, downsample = FALSE, mode='classification')
bs_impVI <- mrvip(
mrBootstrap_obj = bs_fiv ,
yhats = yhats_rf,
X = X,
Y = Y,
mode = 'classification',
threshold = 0.0,
global_top_var = 10,
local_top_var = 5,
taxa = 'pol_132',
ModelPerf = ModelPerf_rf
)
bs_fiv

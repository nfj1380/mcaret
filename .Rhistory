<<<<<<< HEAD
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
=======
pkgdown::build_site()
fl <- mrFlashlight(yhats, X, Y, response = "multi", index=1, model='classification') #index pointing to the SNP of interest (i.e. the first column)
pkgdown::build_site()
detach("package:flashlight", unload = TRUE)
library(flashlight)
knitr::opts_chunk$set(echo = TRUE)
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel); library(parsnip);library(rsample); library(workflows)
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
Y <- FeaturesnoNA #for simplicity
#for more efficient testing for interactions (more variables more interacting pairs)
Y <- FeaturesnoNA[c(1:3)] #three features only
#Optional: Filter rare/common SNPs or species. Retaining minor allele frequencies >0.1 and removing common alleles (occur>0.9)
fData <- filterRareCommon (Responsedata, lower=0.4, higher=0.7)
X <- fData #for simplicity when comparing
X <- X[,-9] #for simplicity when comparing
#another option at this stage is to filter response that are strongly correlated with each other.
#df2 <- cor(X) #find correlations
#hc <-  findCorrelation(df2, cutoff=0.5) # put any value as a "cutoff".
#hc <-  sort(hc)
#X <-  X[,-c(hc)] #
#R <- resist_components(filename = 'location of pairwise matrices', p_val=0.01 ) # p values are used here to filter resistance components that aren't correlated with th original pairwise matrix.
#Y <- cbind(R,Y)
model1 <-
rand_forest(trees = 100, mode = "classification") %>% #100 trees are set for brevity
# select the engine/package that underlies the model
set_engine("ranger", importance = c("impurity","impurity_corrected")) %>%
# choose either the continuous regression or binary classification mode
set_mode("classification")
model1 <-
rand_forest(trees = 100, mode = "classification") %>% #100 trees are set for brevity
# select the engine/package that underlies the model
set_engine("ranger", importance = c("impurity","impurity_corrected")) %>%
# choose either the continuous regression or binary classification mode
set_mode("classification")
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', mod='classification', parallel = TRUE) ## in MrTidymodels. Balanced data= up updamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
VI <- mrVip(yhats, Y=Y)
#plot_vi(VI=VI,X=X, Y=Y, modelPerf=ModelPerf, cutoff= 0, plot.pca='yes', model='classification') #the cutoff reduces the number of individual models printed in the second plot.
groupCov <- c(rep ("Host_characteristics", 1),rep("Urbanisation", 3), rep("Vegetation", 2), rep("Urbanisation",1), rep("Spatial", 2), rep('Host_relatedness', 6),rep ("Host_characteristics", 1),rep("Vegetation", 2), rep("Urbanisation",1))
#plot_vi(VI=VI,  X=X,Y=Y, modelPerf=ModelPerf, groupCov=groupCov, cutoff= 0.5, plot.pca='no')
source(("C:/Users/gmachad/Desktop/mrIML_package/R/mrFlashlight.R"))
#source(("C:/Users/gmachad/Desktop/mrIML_package/R/mrFlashlight.R"))
#source(("C:/Users/gmachad/Desktop/mrIML_package/R/mrProfileplots.R"))
fl <- mrFlashlight(yhats, X, Y, response = "multi", index=1, model='classification') #index pointing to the SNP of interest (i.e. the first column)
plot(light_performance(fl), fill = "orange", rotate_x = TRUE) +
labs(x = element_blank()) +
theme(axis.text.x = element_text(size = 8))
flashlightObj <- mrFlashlight(yhats, X, Y, response = "multi", model='classification')
#plot prediction scatter for all responses. Gets busy with
plot(light_scatter(flashlightObj, v = "Forest", type = "predicted"))
#plots everything on one plot (partial dependency, ALE, scatter)
plot(light_effects(flashlightObj, v = "Grassland"), use = "all")
#profileData_pd <- light_profile(flashlightObj,  v = "Grassland")
#mrProfileplot(profileData_pd , sdthresh =0.05) #sdthresh removes responses from the first plot that do not vary with the feature
profileData_ale <- light_profile(flashlightObj, v = "Grassland", type = "ale") #acumulated local effects
mrProfileplot(profileData_ale , sdthresh =0.01)
#the second plot is the cumulative turnover function
install.packages("pdp")
pkgdown::build_site()
#Load required libraries
library(ggplot2)
library(deSolve)
library(reshape2)
initial_state_values=c(S=999999,I=1,R=0)
parameters=c(gamma=0.20,beta=0.5)
time=seq(from=1,to=100,by=1)
sir_model <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
>>>>>>> dbccd43166391d87671ae0def0ce3a4cc8e88027
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
if (model=='classification'){
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
}
if (model=='regression'){
# select the best model
best_m <- tune_m %>%
select_best("rmse")
}
# Calculate probability predictions for the fitted training data.
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
mod1_k <- final_model %>%
fit(data = data_train)
# Calculate deviance residuals
if (model=='classification'){
#predictions
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid= NULL
}
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#fit on the training set and evaluate on test set.
yhats <- lapply(seq(1,n_response), internal_fit_function)
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
}
}
yhats1 <- mrIMLpredicts(X= X, Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5, seed = sample.int(1e8, 1))
yhats1
internal_fit_function <- function( i ){
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
set.seed(seed)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
if (model=='classification'){
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
}
if (model=='regression'){
# select the best model
best_m <- tune_m %>%
select_best("rmse")
}
# Calculate probability predictions for the fitted training data.
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
mod1_k <- final_model %>%
fit(data = data_train)
# Calculate deviance residuals
if (model=='classification'){
#predictions
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid= NULL
}
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#'Wrapper to generate multi-response predictive models.
#'@param Y A \code{dataframe} is a response variable data set (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param balance_data A \code{character} 'up', 'down' or 'no'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'@param tune_grid_size A \code{numeric} sets the grid size for hyperparamter tuning. Larger grid sizes increase computational time.
#'@details This function produces yhats that used in all model characteristics for subsequent functions.
#' This function fits separate classication models for each response variable in a dataset. Y (response variables) should be binary (0/1). Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imblanace can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#' @example
#' model1 <-
#' rand_forest(trees = 100, mode = "classification") %>% #this should cope with multinomial data alreadf
#'   set_engine("ranger", importance = c("impurity","impurity_corrected")) %>% #model is not tuned to increase computational speed
#'  set_mode("classification")
#'
#' yhats <- mrIMLpredicts(X= venviro_variables,Y=response data, model1=model1, balance_data='no', model='classification', parallel = TRUE,
#'tune_grid_size=5, seed = sample.int(1e8, 1)))
#'@export
mrIMLpredicts<- function(X, Y, model1, balance_data ='no', model='regression', parallel = TRUE, transformY='log', tune_grid_size= 10, seed = sample.int(1e8, 1) ) {
if(parallel==TRUE){
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
}
n_response<- length(X)
# Run model 1 for each parasite; a simple logistic regression with a single covariate
# in this case but note that model 1 can be any model of the user's choice,
# from simple regressions to complex hierarchical or deep learning models.
# Different structures can also be used for each species to handle mixed multivariate outcomes
mod1_perf <- NULL #place to save performance matrix
#yhats <- for(i in 1:length(X)) {
#rhats <- lapply(seq(1, n_variables), function(species){
internal_fit_function <- function( i ){
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
set.seed(seed)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
if (model=='classification'){
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
}
if (model=='regression'){
# select the best model
best_m <- tune_m %>%
select_best("rmse")
}
# Calculate probability predictions for the fitted training data.
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
mod1_k <- final_model %>%
fit(data = data_train)
# Calculate deviance residuals
if (model=='classification'){
#predictions
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid= NULL
}
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
}
#fit on the training set and evaluate on test set.
yhats <- lapply(seq(1,n_response), internal_fit_function)
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
}
}
yhats <- mrIMLpredicts(X= X, Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5, seed = sample.int(1e8, 1))
yhats
#'Wrapper to generate multi-response predictive models.
#'@param Y A \code{dataframe} is a response variable data set (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param balance_data A \code{character} 'up', 'down' or 'no'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'@param tune_grid_size A \code{numeric} sets the grid size for hyperparamter tuning. Larger grid sizes increase computational time.
#'@details This function produces yhats that used in all model characteristics for subsequent functions.
#' This function fits separate classication models for each response variable in a dataset. Y (response variables) should be binary (0/1). Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imblanace can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#' @example
#' model1 <-
#' rand_forest(trees = 100, mode = "classification") %>% #this should cope with multinomial data alreadf
#'   set_engine("ranger", importance = c("impurity","impurity_corrected")) %>% #model is not tuned to increase computational speed
#'  set_mode("classification")
#'
#' yhats <- mrIMLpredicts(X= venviro_variables,Y=response data, model1=model1, balance_data='no', model='classification', parallel = TRUE,
#'tune_grid_size=5, seed = sample.int(1e8, 1)))
#'@export
mrIMLpredicts<- function(X, Y, model1, balance_data ='no', model='regression', parallel = TRUE, transformY='log', tune_grid_size= 10, seed = sample.int(1e8, 1) ) {
if(parallel==TRUE){
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
}
n_response<- length(X)
# Run model 1 for each parasite; a simple logistic regression with a single covariate
# in this case but note that model 1 can be any model of the user's choice,
# from simple regressions to complex hierarchical or deep learning models.
# Different structures can also be used for each species to handle mixed multivariate outcomes
mod1_perf <- NULL #place to save performance matrix
#yhats <- for(i in 1:length(X)) {
#rhats <- lapply(seq(1, n_variables), function(species){
internal_fit_function <- function( i ){
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
set.seed(seed)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
if (model=='classification'){
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
}
if (model=='regression'){
# select the best model
best_m <- tune_m %>%
select_best("rmse")
}
# Calculate probability predictions for the fitted training data.
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
mod1_k <- final_model %>%
fit(data = data_train)
# Calculate deviance residuals
if (model=='classification'){
#predictions
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid= NULL
}
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#save data
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
}
#fit on the training set and evaluate on test set.
yhats <- lapply(seq(1,n_response), internal_fit_function)
}
yhats <- mrIMLpredicts(X= X, Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5, seed = sample.int(1e8, 1))
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf[[1]]
#'Wrapper to calculate performance metrics (Mathews correlation coefficent, sensitivity and specificity) for each model for each response variable.
#'@param yhats A \code{list} is the list generated by mrIMLpredicts
#'@param model1 A \code{list}  #the model used to generate the yhats object
#'@param X  A \code{dataframe} #is a response variable data set (specie, SNPs etc).
#'
#'@example
#' \dontrun{
#' ModelPerf <- mrIMLperformance(yhats, model1, X=X) }
#'
#'@details Outputs a dataframe of commonly used metric that can be used to compare model performance of classification models. Performance metrics are based on testing data. But MCC is useful (higher numbers = better fit)
#'
#'@export
mrIMLperformance <- function(yhats, model1, X, model='regression'){
n_response<- length(yhats)
mod_perf <- NULL
#  yList <- yhats %>% purrr::map(pluck('yhatT')) #get the training yhats all together
bList <- yhats %>% purrr::map(pluck('last_mod_fit')) ## fix the model ID
if (model=='classification'){
for( i in 1:n_response) {
#get already calculated ROC and accuracy
met1 <- as.data.frame(bList[[i]]$.metrics) ####
roc <- met1$.estimate[2]
#accuracy <-  met1$.estimate[1]
#data from best model to get MCC, specificity and sensivity using yardstick
yd <- as.data.frame(bList[[i]]$.predictions) ###
mathews <-  yardstick::mcc(yd,class, .pred_class) #yardstick is the tidymodels performance package.
mathews <- mathews$.estimate #extract mcc
sen <- yardstick::sens(yd,class, .pred_class)
sen <- sen$.estimate
spe <- yardstick::spec(yd,class, .pred_class)
spe <- spe$.estimate
#add some identifiers
mod_name <- class(model1)[1] #extracts model name
#model1_perf <- c(mod_name, mathews, sen, spe)
sp <- names(X[i])
prev <- sum(X[i])/nrow(X)
#save all the metrics
mod_perf[[i]] <- c( sp, mod_name, roc, mathews, sen, spe, prev)
}
mod1_perf<- do.call(rbind, mod_perf)
mod1_perf <- as.data.frame( mod1_perf)
colnames(mod1_perf) <- c('response', 'model_name', 'roc_AUC', 'mcc', 'sensitivity', 'specificity', 'prevalence')
Global_summary<- as.numeric(as.character(unlist(mod1_perf$roc_AUC))) #cant be mcc as it goes negative
Global_summary[is.na(Global_summary)] <- 0
Global_summary <-  mean(Global_summary)
}
if (model=='regression'){
for( i in 1:n_response) {
met1 <- as.data.frame(bList[[i]]$.metrics) ####
rmse <- met1$.estimate[1]
rsq  <- met1$.estimate[2]
#add some identifiers
mod_name <- class(model1)[1] #extracts model name
sp <- names(X[i])
#save all the metrics
# mod_perf[[i]] <- c( sp, mod_name, rmse, rsq)
mod_perf[[i]] <- data.frame( sp, mod_name, rmse, rsq)
}
mod1_perf<- do.call(rbind, mod_perf)
mod1_perf <- as.data.frame( mod1_perf)
colnames(mod1_perf) <- c('response', 'model_name', 'rmse', 'rsquared')
Global_summary<- as.numeric(as.character(unlist(mod1_perf$rmse)))
#Global_summary[is.na(Global_summary)] <- 0
#Global_summary <-  mean(Global_summary)
Global_summary <-  mean(Global_summary, na.rm = TRUE)
}
#calculate mean for all response variables
#Global_summary <- mean(as.numeric(as.character(unlist( mod1_perf[[5]]))), na.rm = FALSE) #mean testing mcc
#Global_summary <- mean(as.numeric(as.character(unlist( ModelPerf[[1]]$mcc)))) #mean testing
#
return (list(mod1_perf, Global_summary))
}
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf[[1]]
devtools:: document()
rm(list = c("mrIMLperformance", "mrIMLpredicts"))`
)
<<<<<<< HEAD
'
''
""
`
rm(list = c("mrIMLperformance", "mrIMLpredicts"))
devtools:: document()
devtools:: document()
=======
library(LEA)
?gfData
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis); library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel)
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("LEA")
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis); library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel)
#library(LEA)
# read in data file with minor allele freqs & env/space variables
gfData; str(gfData)
envGF <- gfData[,3:13] # get climate & MEM variables
Y <- envGF #for simplicity
# build individual SNP datasets
SNPs_ref <- gfData[,grep("REFERENCE",colnames(gfData))] # reference
GI5 <- gfData[,grep("GI5",colnames(gfData))] # GIGANTEA-5 (GI5)
X <- GI5 #for this example we are going to focus on the adaptive SNPs in the GI5 region.
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
gfData; str(gfData)
str()
str
?str\
?str
pkgdown::build_site()
build_home()
build_news()
pkgdown::build_site()
pkgdown::build_site()
library(pkgdown)
pkgdown::build_site()
build_news()
#init_site()
sink()
#init_site()
sink()
build_news()
pkgdown::build_site()
library(pkgdown)
#init_site()
sink()
pkgdown::build_site()
library(pkgdown)
pkgdown::build_site()
usethis::use_github_action("pkgdown")
usethis::use_github_action("pkgdown")
pkgdown::build_site()
getwd()
build_news()
library(pkgdown)
build_home()
pkgdown::build_site()
library(pkgdown)
library(pkgdown)
pkgdown::build_site()
>>>>>>> dbccd43166391d87671ae0def0ce3a4cc8e88027

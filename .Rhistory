<<<<<<< HEAD
level=5)
#'Wrapper to generate multi-response predictive models.
#'@param Y A \code{dataframe} is a response variable data set (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param balance_data A \code{character} 'up', 'down' or 'no'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'
#'@examples
#'model1 <- #model used to generate yhat
#'specify that the model is a random forest
#'logistic_reg() %>%
#' # select the engine/package that underlies the model
#'set_engine("glm") %>%
#'  # choose either the continuous regression or binary classification mode
#'  set_mode("classification")
#'@details This function produces yhats that used in all model characteristics for subsequent functions.
#' This function fits separate classication models for each response variable in a dataset. Y (response variables) should be binary (0/1). Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imblanace can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#'@export
mrIMLpredicts<- function(X, Y, model1, balance_data ='no', model='regression', parallel = TRUE, transformY='log'){#tune_grid_size= 10 ) {
if(parallel==TRUE){
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
}
n_response<- length(X)
# Run model 1 for each parasite; a simple logistic regression with a single covariate
# in this case but note that model 1 can be any model of the user's choice,
# from simple regressions to complex hierarchical or deep learning models.
# Different structures can also be used for each species to handle mixed multivariate outcomes
mod1_perf <- NULL #place to save performance matrix
#yhats <- for(i in 1:length(X)) {
yhats <- lapply(seq(1,n_response), function(i){
#rhats <- lapply(seq(1, n_variables), function(species){
#not needed for this model
#OtherSNPs <- as.data.frame(X[-1])
#OtherSNPs[OtherSNPs=='Negative'] <- 0
#OtherSNPs[OtherSNPs== 'Positive'] <- 1 #could do a PCA/PCoA?
#OtherSNPsa <-apply(OtherSNPs, 2, as.numeric)
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
#data<-data[complete.cases(data)] #removes NAs but there must be a conflict somewhere
set.seed(100)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
# extract training and testing sets stata
#data_trainalt <- training(data_splitalt)
#data_testalt <- testing(data_splitalt)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
if (model=='classification'){
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size) # 10 paramters
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
# final
final_model <- finalize_workflow(mod_workflow),
best_m )
# Fit model one for each parasite; can easily modify this so that the user
# can specify the formula necessary for each species as a list of formulas
mod1_k <- final_model %>%
fit(data = data_train)
#mod1_k %>%
#fit_resamples(resamples = data_cv)
# keep the tune all list
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#fit on the training set and evaluate on test set. Not needed
#last_fit(data_split)
# Calculate probability predictions for the fitted training data.
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
# Calculate deviance residuals
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = 10) # 10 paramters
# select the best model
best_m <- tune_m %>%
select_best("rmse") #
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
# Fit model one for each parasite; can easily modify this so that the user
# can specify the formula necessary for each species as a list of formulas
mod1_k <- final_model %>%
fit(data = data_train)
#mod1_k %>%
#fit_resamples(resamples = data_cv)
# keep the tune all list
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#fit on the training set and evaluate on test set. Not needed
#last_fit(data_split)
# Calculate probability predictions for the fitted training data.
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid <- NA
}
#list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
})
}
#'Wrapper to generate multi-response predictive models.
#'@param Y A \code{dataframe} is a response variable data set (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param balance_data A \code{character} 'up', 'down' or 'no'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'
#'@examples
#'model1 <- #model used to generate yhat
#'specify that the model is a random forest
#'logistic_reg() %>%
#' # select the engine/package that underlies the model
#'set_engine("glm") %>%
#'  # choose either the continuous regression or binary classification mode
#'  set_mode("classification")
#'@details This function produces yhats that used in all model characteristics for subsequent functions.
#' This function fits separate classication models for each response variable in a dataset. Y (response variables) should be binary (0/1). Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imblanace can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#'@export
mrIMLpredicts<- function(X, Y, model1, balance_data ='no', model='regression', parallel = TRUE, transformY='log'){#tune_grid_size= 10 ) {
if(parallel==TRUE){
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
}
n_response<- length(X)
# Run model 1 for each parasite; a simple logistic regression with a single covariate
# in this case but note that model 1 can be any model of the user's choice,
# from simple regressions to complex hierarchical or deep learning models.
# Different structures can also be used for each species to handle mixed multivariate outcomes
mod1_perf <- NULL #place to save performance matrix
#yhats <- for(i in 1:length(X)) {
yhats <- lapply(seq(1,n_response), function(i){
#rhats <- lapply(seq(1, n_variables), function(species){
#not needed for this model
#OtherSNPs <- as.data.frame(X[-1])
#OtherSNPs[OtherSNPs=='Negative'] <- 0
#OtherSNPs[OtherSNPs== 'Positive'] <- 1 #could do a PCA/PCoA?
#OtherSNPsa <-apply(OtherSNPs, 2, as.numeric)
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
#data<-data[complete.cases(data)] #removes NAs but there must be a conflict somewhere
set.seed(100)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
# extract training and testing sets stata
#data_trainalt <- training(data_splitalt)
#data_testalt <- testing(data_splitalt)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
if (model=='classification'){
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
# Fit model one for each parasite; can easily modify this so that the user
# can specify the formula necessary for each species as a list of formulas
mod1_k <- final_model %>%
fit(data = data_train)
#mod1_k %>%
#fit_resamples(resamples = data_cv)
# keep the tune all list
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#fit on the training set and evaluate on test set. Not needed
#last_fit(data_split)
# Calculate probability predictions for the fitted training data.
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
# Calculate deviance residuals
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
# select the best model
best_m <- tune_m %>%
select_best("rmse") #
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
# Fit model one for each parasite; can easily modify this so that the user
# can specify the formula necessary for each species as a list of formulas
mod1_k <- final_model %>%
fit(data = data_train)
#mod1_k %>%
#fit_resamples(resamples = data_cv)
# keep the tune all list
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#fit on the training set and evaluate on test set. Not needed
#last_fit(data_split)
# Calculate probability predictions for the fitted training data.
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid <- NA
}
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
})
}
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5)
#'Wrapper to generate multi-response predictive models.
#'@param Y A \code{dataframe} is a response variable data set (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param balance_data A \code{character} 'up', 'down' or 'no'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'
#'@examples
#'model1 <- #model used to generate yhat
#'specify that the model is a random forest
#'logistic_reg() %>%
#' # select the engine/package that underlies the model
#'set_engine("glm") %>%
#'  # choose either the continuous regression or binary classification mode
#'  set_mode("classification")
#'@details This function produces yhats that used in all model characteristics for subsequent functions.
#' This function fits separate classication models for each response variable in a dataset. Y (response variables) should be binary (0/1). Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imblanace can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#'@export
mrIMLpredicts<- function(X, Y, model1, balance_data ='no', model='regression', parallel = TRUE, transformY='log', tune_grid_size= 10 ) {
if(parallel==TRUE){
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
}
n_response<- length(X)
# Run model 1 for each parasite; a simple logistic regression with a single covariate
# in this case but note that model 1 can be any model of the user's choice,
# from simple regressions to complex hierarchical or deep learning models.
# Different structures can also be used for each species to handle mixed multivariate outcomes
mod1_perf <- NULL #place to save performance matrix
#yhats <- for(i in 1:length(X)) {
yhats <- lapply(seq(1,n_response), function(i){
#rhats <- lapply(seq(1, n_variables), function(species){
#not needed for this model
#OtherSNPs <- as.data.frame(X[-1])
#OtherSNPs[OtherSNPs=='Negative'] <- 0
#OtherSNPs[OtherSNPs== 'Positive'] <- 1 #could do a PCA/PCoA?
#OtherSNPsa <-apply(OtherSNPs, 2, as.numeric)
data <- cbind(X[i], Y) ###
colnames(data)[1] <- c('class') #define response variable
if (model=='classification'){
data$class<- as.factor(data$class)}
#data<-data[complete.cases(data)] #removes NAs but there must be a conflict somewhere
set.seed(100)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
# extract training and testing sets stata
#data_trainalt <- training(data_splitalt)
#data_testalt <- testing(data_splitalt)
#10 fold cross validation
data_cv <- vfold_cv(data_train, v= 10)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>% #if downsampling is needed
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>% #data imbalance not corrected. This has to be the option for regression problems
recipe(class ~., data= data_train)
}
if ( class(model1)[1] == 'logistic_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( class(model1)[1] == 'linear_reg'){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(model1)
## full tunning
if (model=='classification'){
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
# Fit model one for each parasite; can easily modify this so that the user
# can specify the formula necessary for each species as a list of formulas
mod1_k <- final_model %>%
fit(data = data_train)
#mod1_k %>%
#fit_resamples(resamples = data_cv)
# keep the tune all list
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#fit on the training set and evaluate on test set. Not needed
#last_fit(data_split)
# Calculate probability predictions for the fitted training data.
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
# Calculate deviance residuals
resid <- devianceResids(yhatO, data_train$class)
}
if (model=='regression'){
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
# select the best model
best_m <- tune_m %>%
select_best("rmse")
# final
final_model <- finalize_workflow(mod_workflow,
best_m )
# Fit model one for each parasite; can easily modify this so that the user
# can specify the formula necessary for each species as a list of formulas
mod1_k <- final_model %>%
fit(data = data_train)
#mod1_k %>%
#fit_resamples(resamples = data_cv)
# keep the tune all list
# the last fit
set.seed(345)
last_mod_fit <-
final_model %>%
last_fit(data_split)
#fit on the training set and evaluate on test set. Not needed
#last_fit(data_split)
# Calculate probability predictions for the fitted training data.
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid <- NA
}
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
})
}
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5) ## in MrTidymodels. Balanced data= up updamples and down downsampled to create a balanced set. For regression there no has to be selected.
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf[[1]]
ModelPerf[[2]]
X <- select(Bird.parasites, -scale.prop.zos) #response variables eg. SNPs, pathogens, species....
Y <- select(Bird.parasites, scale.prop.zos) # feature set
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,  tune_grid_size=5) ## in MrTidymodels. Balanced data= up updamples and down downsampled to create a balanced set. For regression there no has to be selected.
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf[[1]]
model1 <-
boost_tree( trees = 100,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(),                     ## first three: model complexity
sample_size = tune(), mtry = tune(),         ## randomness
learn_rate = tune()# step size
) %>%
set_engine("xgboost") %>%
set_mode("classification")
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,
tune_grid_size=5)
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf
model1 <-
boost_tree( trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(),                     ## first three: model complexity
sample_size = tune(), mtry = tune(),         ## randomness
learn_rate = tune()# step size
) %>%
set_engine("xgboost") %>%
set_mode("classification")
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', model='classification', parallel = TRUE,
tune_grid_size=5) ## in MrTidymodels. Balanced data= up updamples and down downsampled to create a balanced set. For regression there no has to be selected.
ModelPerf <- mrIMLperformance(yhats, model1, X=X, model='classification')
ModelPerf[[1]]
=======
```{r message = FALSE, warning = FALSE}
fl <- mrFlashlight(yhats, X, Y, response = "multi", index=1, model='classification') #index pointing to the SNP of interest (i.e. the first column)
plot(light_performance(fl), fill = "orange", rotate_x = TRUE) +
labs(x = element_blank()) +
theme(axis.text.x = element_text(size = 8))
```
We also wrap some flashlight functionality to visualize the marginal (i.e. partial dependencies) or conditional (accumulated local effects) effect of a feature on genetic change. Partial dependencies take longer to calculate and are more sensitive to correlated features. The first plot You can see here that they can get different results based on which plot you use. ALE plots are a better option if your feature set is even moderately impacted by collinearity ( e.g., ., rho = 0.6). The second plot is the overall smoothed genetic-turnover function.
When running this code yourself, you need to change 'v' to the variable of interest within your Y dataset.In this case we are looking at how grassland shapes genetic turnover.
```{r, message = FALSE, warning = FALSE}
flashlightObj <- mrFlashlight(yhats, X, Y, response = "multi", model='classification')
#plot prediction scatter for all responses. Gets busy with
plot(light_scatter(flashlightObj, v = "Forest", type = "predicted"))
#plots everything on one plot (partial dependency, ALE, scatter)
plot(light_effects(flashlightObj, v = "Grassland"), use = "all")
#profileData_pd <- light_profile(flashlightObj,  v = "Grassland")
#mrProfileplot(profileData_pd , sdthresh =0.05) #sdthresh removes responses from the first plot that do not vary with the feature
profileData_ale <- light_profile(flashlightObj, v = "Grassland", type = "ale") #acumulated local effects
mrProfileplot(profileData_ale , sdthresh =0.01)
#the second plot is the cumulative turnover function
```
Finally, we can assess how features interact overall to shape genetic change. Be warned this is memory intensive. Future updates to this package will enable users to visualize these interactions and explore them in more detail using 2D ALE plots for example.
```{r message = FALSE, warning = FALSE}
interactions <-mrInteractions(yhats, X, Y,  mod='classification') #this is computationally intensive so multicores are needed. If stopped prematurely - have to reload things
mrPlot_interactions(interactions, X,Y, top_ranking = 2, top_response=2)
#save(interactions, file='Fitzpatrick2016interactions')
```
You could easily compare the plots generated above from our random forest model with the logistic regression model we also calculated.
This is just an example of the tools can be applied to better interpret multi-response models and in turn gain insights into how landscape impacts genetic change. We've demonstrated that this package is modular  and flexible, allowing future work on 'tidymodels' and interpretable machine learning to easily be incorporated. The predictions from these models,  for example,  can be used to identify individual SNPs that may be adaptive assess gene flow across space (future updates) or to help better formulate probabilistic models such as generalized dissimilarity models (GDMs).
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel); library(parsnip);library(rsample); library(workflows)
library(LEA)
install.packages("BiocManager")
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel); library(parsnip);library(rsample); library(workflows)
library(LEA)
if (!requireNamespace("BiocManager", quietly = TRUE))
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel); library(parsnip);library(rsample); library(workflows)
library(LEA)
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel); library(parsnip);library(rsample); library(workflows)
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
Y <- FeaturesnoNA #for simplicity
#for more efficient testing for interactions (more variables more interacting pairs)
Y <- FeaturesnoNA[c(1:3)] #three features only
pkgdown::build_site()
fl <- mrFlashlight(yhats, X, Y, response = "multi", index=1, model='classification') #index pointing to the SNP of interest (i.e. the first column)
pkgdown::build_site()
detach("package:flashlight", unload = TRUE)
library(flashlight)
knitr::opts_chunk$set(echo = TRUE)
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels); library(randomForest);  library(caret); library(gbm);
library(tidyverse);library(parallel); library(doParallel); library(themis); library(viridis);
library(janitor); library(hrbrthemes); library(xgboost); library(vegan);library(flashlight);
library(ggrepel); library(parsnip);library(rsample); library(workflows)
#load SNP data
#Responsedata)
#if you have a plink dataset you can load it in to our pipeline with the following:
#snps <- readSnpsPed("snp.ped", "snp.map") #NAs in data and interpolated as the mode.
#landscape and host features (or predictors). Note that samples must be rows.
str(Features)
# # remove NAs from the feature/predictor data.
FeaturesnoNA<-Features[complete.cases(Features), ]
Y <- FeaturesnoNA #for simplicity
#for more efficient testing for interactions (more variables more interacting pairs)
Y <- FeaturesnoNA[c(1:3)] #three features only
#Optional: Filter rare/common SNPs or species. Retaining minor allele frequencies >0.1 and removing common alleles (occur>0.9)
fData <- filterRareCommon (Responsedata, lower=0.4, higher=0.7)
X <- fData #for simplicity when comparing
X <- X[,-9] #for simplicity when comparing
#another option at this stage is to filter response that are strongly correlated with each other.
#df2 <- cor(X) #find correlations
#hc <-  findCorrelation(df2, cutoff=0.5) # put any value as a "cutoff".
#hc <-  sort(hc)
#X <-  X[,-c(hc)] #
#R <- resist_components(filename = 'location of pairwise matrices', p_val=0.01 ) # p values are used here to filter resistance components that aren't correlated with th original pairwise matrix.
#Y <- cbind(R,Y)
model1 <-
rand_forest(trees = 100, mode = "classification") %>% #100 trees are set for brevity
# select the engine/package that underlies the model
set_engine("ranger", importance = c("impurity","impurity_corrected")) %>%
# choose either the continuous regression or binary classification mode
set_mode("classification")
model1 <-
rand_forest(trees = 100, mode = "classification") %>% #100 trees are set for brevity
# select the engine/package that underlies the model
set_engine("ranger", importance = c("impurity","impurity_corrected")) %>%
# choose either the continuous regression or binary classification mode
set_mode("classification")
yhats <- mrIMLpredicts(X=X,Y=Y, model1=model1, balance_data='no', mod='classification', parallel = TRUE) ## in MrTidymodels. Balanced data= up updamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
VI <- mrVip(yhats, Y=Y)
#plot_vi(VI=VI,X=X, Y=Y, modelPerf=ModelPerf, cutoff= 0, plot.pca='yes', model='classification') #the cutoff reduces the number of individual models printed in the second plot.
groupCov <- c(rep ("Host_characteristics", 1),rep("Urbanisation", 3), rep("Vegetation", 2), rep("Urbanisation",1), rep("Spatial", 2), rep('Host_relatedness', 6),rep ("Host_characteristics", 1),rep("Vegetation", 2), rep("Urbanisation",1))
#plot_vi(VI=VI,  X=X,Y=Y, modelPerf=ModelPerf, groupCov=groupCov, cutoff= 0.5, plot.pca='no')
source(("C:/Users/gmachad/Desktop/mrIML_package/R/mrFlashlight.R"))
#source(("C:/Users/gmachad/Desktop/mrIML_package/R/mrFlashlight.R"))
#source(("C:/Users/gmachad/Desktop/mrIML_package/R/mrProfileplots.R"))
fl <- mrFlashlight(yhats, X, Y, response = "multi", index=1, model='classification') #index pointing to the SNP of interest (i.e. the first column)
plot(light_performance(fl), fill = "orange", rotate_x = TRUE) +
labs(x = element_blank()) +
theme(axis.text.x = element_text(size = 8))
flashlightObj <- mrFlashlight(yhats, X, Y, response = "multi", model='classification')
#plot prediction scatter for all responses. Gets busy with
plot(light_scatter(flashlightObj, v = "Forest", type = "predicted"))
#plots everything on one plot (partial dependency, ALE, scatter)
plot(light_effects(flashlightObj, v = "Grassland"), use = "all")
#profileData_pd <- light_profile(flashlightObj,  v = "Grassland")
#mrProfileplot(profileData_pd , sdthresh =0.05) #sdthresh removes responses from the first plot that do not vary with the feature
profileData_ale <- light_profile(flashlightObj, v = "Grassland", type = "ale") #acumulated local effects
mrProfileplot(profileData_ale , sdthresh =0.01)
#the second plot is the cumulative turnover function
install.packages("pdp")
pkgdown::build_site()
#Load required libraries
library(ggplot2)
library(deSolve)
library(reshape2)
initial_state_values=c(S=999999,I=1,R=0)
parameters=c(gamma=0.20,beta=0.5)
time=seq(from=1,to=100,by=1)
sir_model <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
}
)
}
#Solving the differential equations
output<-as.data.frame(ode(y=initial_state_values,func = sir_model,parms=parameters,times = time))
#Load required libraries
library(ggplot2)
library(deSolve)
install.packages("deSolve")
#Load required libraries
library(ggplot2)
library(deSolve)
library(reshape2)
# Model inputs
initial_state_values=c(S=999999,I=1,R=0)
parameters=c(gamma=0.20,beta=0.5)
# Time points
time=seq(from=1,to=100,by=1)
# SIR model function
sir_model <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
}
)
}
#Solving the differential equations
output<-as.data.frame(ode(y=initial_state_values,func = sir_model,parms=parameters,times = time))
out_long=melt(output,id="time")
# To plot the proportion of susceptible, infected and recovered individuals over time
ggplot(data = out_long,
aes(x = time, y = value/1000000, colour = variable, group = variable)) +
geom_line() +xlab("Time (days)")+ylab("Proportion of the population")+
scale_color_discrete(name="State")
library(ggplot2)
library(deSolve)
library(reshape2)
# Model inputs
initial_state_values=c(S=999999,I=1,R=0)
parameters=c(gamma=0.2*365,beta=0.4*365,mu=1/70,b=1/70)
# Time points
time=seq(from=1,to=400,by=1/365)
# SIR model function
sir_model2 <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S-mu*S+b*N
dI=lambda*S-gamma*I-mu*I
dR=gamma*I-mu*R
return(list(c(dS,dI,dR)))
}
)
}
# Solving the differential equations:
output<-as.data.frame(ode(y=initial_state_values,func = sir_model2,parms=parameters,times = time))
out_long=melt(output,id="time")
#Plotting the prevelance over time
ggplot(data = out_long,
aes(x = time, y = value/1000000, colour = variable, group = variable)) +
geom_line() +
xlab("Time (years)")+
ylab("Prevalence") + scale_color_discrete(name="State")
#Loading libraries
library(ggplot2)
library(deSolve)
library(reshape2)
# Model inputs
initial_state_values=c(S=0.60*999999,I=1,R=0.40*999999)
parameters=c(gamma=0.1,beta=0.4)
# Time points
time=seq(from=1,to=3*365,by=1)
#SIR Model function
sir_model3 <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
}
)
}
# Solving the differential equations:
output<-as.data.frame(ode(y=initial_state_values,func = sir_model3,parms=parameters,times = time))
out_long=melt(output,id="time")
#Plot of prevalance
ggplot(data = out_long,
aes(x = time, y = value/1000000, colour = variable, group = variable)) +
geom_line() +
xlab("Time (days)")+
ylab("Prevalance") +scale_color_discrete(name="State")
parameters=c(gamma=0.1,beta=0.75)
time=seq(from=1,to=3*365,by=1)
#SIR Model function
sir_model3 <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
}
)
}
# Solving the differential equations:
output<-as.data.frame(ode(y=initial_state_values,func = sir_model3,parms=parameters,times = time))
out_long=melt(output,id="time")
#Plot of prevalance
ggplot(data = out_long,
aes(x = time, y = value/1000000, colour = variable, group = variable)) +
geom_line() +
xlab("Time (days)")+
ylab("Prevalance") +scale_color_discrete(name="State")
initial_state_values=c(S=0.60*999999,I=1,R=0.75*999999)
parameters=c(gamma=0.1,beta=0.4)
time=seq(from=1,to=3*365,by=1)
#SIR Model function
sir_model3 <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
}
)
}
# Solving the differential equations:
output<-as.data.frame(ode(y=initial_state_values,func = sir_model3,parms=parameters,times = time))
out_long=melt(output,id="time")
#Plot of prevalance
ggplot(data = out_long,
aes(x = time, y = value/1000000, colour = variable, group = variable)) +
geom_line() +
xlab("Time (days)")+
ylab("Prevalance") +scale_color_discrete(name="State")
#Loading libraries
library(ggplot2)
library(deSolve)
library(reshape2)
# Model inputs
initial_state_values=c(S=0.60*999999,I=1,R=0.40*999999)
parameters=c(gamma=0.1,beta=0.4)
# Time points
time=seq(from=1,to=3*365,by=1)
#SIR Model function
sir_model3 <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
}
)
}
# Solving the differential equations:
output<-as.data.frame(ode(y=initial_state_values,func = sir_model3,parms=parameters,times = time))
out_long=melt(output,id="time")
#Plot of prevalance
ggplot(data = out_long,
aes(x = time, y = value/1000000, colour = variable, group = variable)) +
geom_line() +
xlab("Time (days)")+
ylab("Prevalance") +scale_color_discrete(name="State")
parameters=c(gamma=0.1,beta=0.7)
time=seq(from=1,to=3*365,by=1)
#SIR Model function
sir_model3 <- function(time,state,parameters){
with(as.list(c(state,parameters)),{
N=S+I+R
lambda=beta*(I/N)
dS=-lambda*S
dI=lambda*S-gamma*I
dR=gamma*I
return(list(c(dS,dI,dR)))
}
)
}
# Solving the differential equations:
output<-as.data.frame(ode(y=initial_state_values,func = sir_model3,parms=parameters,times = time))
out_long=melt(output,id="time")
#Plot of prevalance
ggplot(data = out_long,
aes(x = time, y = value/1000000, colour = variable, group = variable)) +
geom_line() +
xlab("Time (days)")+
ylab("Prevalance") +scale_color_discrete(name="State")
# Vaccine coverage
p <- 0.75
# Vector storing the initial number of people in each compartment (at timestep 0)
initial_state_values <- c(S = (1-p)*(N-1),   # a proportion 1-p of the total population is susceptible
I = 1,             # the epidemic starts with a single infected person
R = p*(N-1))       # a proportion p of the total population is vaccinated/immune
# Solving the differential equations using the ode integration algorithm
output <- as.data.frame(ode(y = initial_state_values,
times = times,
func = sir_model,
parms = parameters))
# LOAD THE PACKAGES:
library(deSolve)
library(reshape2)
library(ggplot2)
# MODEL INPUTS:
# Vaccine coverage
p <- 0.5
# Total population size
N <- 10^6
# Vector storing the initial number of people in each compartment (at timestep 0)
initial_state_values <- c(S = (1-p)*(N-1),   # a proportion 1-p of the total population is susceptible
I = 1,             # the epidemic starts with a single infected person
R = p*(N-1))       # a proportion p of the total population is vaccinated/immune
# Vector storing the parameters describing the transition rates in units of days^-1
parameters <- c(beta = 0.4,      # the infection rate, which acts on susceptibles
gamma = 0.1)     # the rate of recovery, which acts on those infected
# TIMESTEPS:
# Vector storing the sequence of timesteps to solve the model at
times <- seq(from = 0, to = 730, by = 1)   # from 0 to 730 days in daily intervals
# SIR MODEL FUNCTION:
# The model function takes as input arguments (in the following order): time, state and parameters
sir_model <- function(time, state, parameters) {
with(as.list(c(state, parameters)), {  # tell R to look for variable names within the state and parameters objects
# Calculating the total population size N (the sum of the number of people in each compartment)
N <- S+I+R
# Defining lambda as a function of beta and I:
lambda <- beta * I/N
# The differential equations
dS <- -lambda * S               # people move out of (-) the S compartment at a rate lambda (force of infection)
dI <- lambda * S - gamma * I    # people move into (+) the I compartment from S at a rate lambda,
# and move out of (-) the I compartment at a rate gamma (recovery)
dR <- gamma * I                 # people move into (+) the R compartment from I at a rate gamma
# Return the number of people in the S, I and R compartments at each timestep
# (in the same order as the input state variables)
return(list(c(dS, dI, dR)))
})
}
# MODEL OUTPUT (solving the differential equations):
# Solving the differential equations using the ode integration algorithm
output <- as.data.frame(ode(y = initial_state_values,
times = times,
func = sir_model,
parms = parameters))
output_long <- melt(as.data.frame(output), id = "time")                  # turn output dataset into long format
# Adding a column for the prevalence proportion to the long-format output
output_long$prevalence <- output_long$value/sum(initial_state_values)
# Plot the prevalence proportion
ggplot(data = output_long,                                               # specify object containing data to plot
aes(x = time, y = prevalence, colour = variable, group = variable)) +  # assign columns to axes and groups
geom_line() +                                                          # represent data as lines
xlab("Time (days)")+                                                   # add label for x axis
ylab("Prevalence (proportion)") +                                      # add label for y axis
labs(colour = "Compartment",                                           # add legend title
title = "Prevalence of infection, susceptibility and recovery over time")   # add plot title
# Vaccine coverage
p <- 0.75
# Vector storing the initial number of people in each compartment (at timestep 0)
initial_state_values <- c(S = (1-p)*(N-1),   # a proportion 1-p of the total population is susceptible
I = 1,             # the epidemic starts with a single infected person
R = p*(N-1))       # a proportion p of the total population is vaccinated/immune
# Solving the differential equations using the ode integration algorithm
output <- as.data.frame(ode(y = initial_state_values,
times = times,
func = sir_model,
parms = parameters))
output_long <- melt(as.data.frame(output), id = "time")                  # turn output dataset into long format
# Adding a column for the prevalence proportion to the long-format output
output_long$prevalence <- output_long$value/sum(initial_state_values)
# Plot the prevalence proportion
ggplot(data = output_long,                                               # specify object containing data to plot
aes(x = time, y = prevalence, colour = variable, group = variable)) +  # assign columns to axes and groups
geom_line() +                                                          # represent data as lines
xlab("Time (days)")+                                                   # add label for x axis
ylab("Prevalence (proportion)") +                                      # add label for y axis
labs(colour = "Compartment",                                           # add legend title
title = "Prevalence of infection, susceptibility and recovery over time")   # add plot title
# Vaccine coverage
p <- 0.50
# Vector storing the initial number of people in each compartment (at timestep 0)
initial_state_values <- c(S = (1-p)*(N-1),   # a proportion 1-p of the total population is susceptible
I = 1,             # the epidemic starts with a single infected person
R = p*(N-1))       # a proportion p of the total population is vaccinated/immune
# Solving the differential equations using the ode integration algorithm
output <- as.data.frame(ode(y = initial_state_values,
times = times,
func = sir_model,
parms = parameters))
output_long <- melt(as.data.frame(output), id = "time")                  # turn output dataset into long format
# Adding a column for the prevalence proportion to the long-format output
output_long$prevalence <- output_long$value/sum(initial_state_values)
# Plot the prevalence proportion
ggplot(data = output_long,                                               # specify object containing data to plot
aes(x = time, y = prevalence, colour = variable, group = variable)) +  # assign columns to axes and groups
geom_line() +                                                          # represent data as lines
xlab("Time (days)")+                                                   # add label for x axis
ylab("Prevalence (proportion)") +                                      # add label for y axis
labs(colour = "Compartment",                                           # add legend title
title = "Prevalence of infection, susceptibility and recovery over time")   # add plot title
# Vaccine coverage
p <- 0.72
# Vector storing the initial number of people in each compartment (at timestep 0)
initial_state_values <- c(S = (1-p)*(N-1),   # a proportion 1-p of the total population is susceptible
I = 1,             # the epidemic starts with a single infected person
R = p*(N-1))       # a proportion p of the total population is vaccinated/immune
# Solving the differential equations using the ode integration algorithm
output <- as.data.frame(ode(y = initial_state_values,
times = times,
func = sir_model,
parms = parameters))
output_long <- melt(as.data.frame(output), id = "time")                  # turn output dataset into long format
# Adding a column for the prevalence proportion to the long-format output
output_long$prevalence <- output_long$value/sum(initial_state_values)
# Plot the prevalence proportion
ggplot(data = output_long,                                               # specify object containing data to plot
aes(x = time, y = prevalence, colour = variable, group = variable)) +  # assign columns to axes and groups
geom_line() +                                                          # represent data as lines
xlab("Time (days)")+                                                   # add label for x axis
ylab("Prevalence (proportion)") +                                      # add label for y axis
labs(colour = "Compartment",                                           # add legend title
title = "Prevalence of infection, susceptibility and recovery over time")   # add plot title
>>>>>>> 6182cbec7f93fd3d35fb8a0cd0c7bed112d27662

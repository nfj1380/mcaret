rand_forest(trees = 100, mode = "classification", mtry = tune(), min_n = tune()) %>% #100 trees are set for brevity. Aim to start with 1000
set_engine("randomForest")
#we are tuning mtry and min_n
#set up parallel processing. If you don't add the code below it will still work but just on one core.
## detectCores() #check how many cores you have available. We suggest keeping one core free for internet browsing etc.
##cl <- parallel::makeCluster(4)
##plan(cluster, workers=cl)
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
balance_data='no',
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
cl <- parallel::makeCluster(4)
plan(cluster, workers=cl)
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
library(future.apply)
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
cl <- parallel::makeCluster(4)
plan(cluster, workers=cl)
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
#'Wrapper to estimate model-agnostic variable importance for multi-response models.
#'@param yhats A \code{list} is the list generated by mrIMLpredicts
#'@param X  A \code{dataframe} is the feature data
#'@details Calculates variable importance using based on partial dependencies but could do permutations as well (more memory intensive).
#' Key input is the object created by MrIML. Can be plotted with the plot_vi function.
#' @example
#' VI <- mrVip(yhats, X=X)
#' groupCov <- c(rep ("Host_characteristics", 1),rep("Urbanisation", 3), rep("Vegetation", 2), rep("Urbanisation",1), rep("Spatial", 2),
#' rep('Host_relatedness', 6),rep ("Host_characteristics", 1),rep("Vegetation", 2), rep("Urbanisation",1))
#' plot_vi(VI=VI,  X=X,Y=Y, modelPerf=ModelPerf, groupCov, cutoff= 0.5)
#'@export
mrVip_mutlirun <- function (X, Y, Model,
mode='regression',ity=5, transformY='log',
dummy=FALSE, tune_grid_size= 10, k=10,
seed = sample.int(1e8, 1) ) {
internal_fit_function <- function( i ){
yhats_rf <- mrIMLpredicts(X=X,
Y=Y,
Model=model_rf,
balance_data='no',
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1) ) #this will make sure dif seeds are used
VI <- mrVip(yhats_rf, X=X)
}
im <- future.apply::future_lapply(seq(1,ity), internal_fit_function, future.seed = TRUE)
ImpGlobal <- as.data.frame(do.call(rbind, im)) #from cbind
}
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
cl <- parallel::makeCluster(4)
plan(cluster, workers=cl)
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
library(future.apply)
cl <- parallel::makeCluster(4)
plan(cluster, workers=cl)
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
#'Wrapper to generate multi-response predictive models.
#'@param Y A \code{dataframe} is response variable data (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param balance_data A \code{character} 'up', 'down' or 'no'.
#'@param dummy A \code{logical} 'TRUE or FALSE'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'@param tune_grid_size A \code{numeric} sets the grid size for hyperparamter tuning. Larger grid sizes increase computational time.
#'@param k A \code{numeric} sets the number of folds in the 10-fold cross-validation. 10 is the default.
#'@seed A \code{numeric} as these models have a stochastic component, a seed is set to make to make the analysis reproducible. Defaults between 100 million and 1.
#'@param mode \code{character}'classification' or 'regression' i.e., is the generative model a regression or classification?
#'@details This function produces yhats that used in all subsequent functions.
#' This function fits separate classification/regression models for each response variable in a data set.  Rows in X (features) have the same id (host/site/population)
#'  as Y. Class imbalance can be a real issue for classification analyses. Class imbalance can be addressed for each
#' response variable using 'up' (upsampling using ROSE bootstrapping), 'down' (downsampling)
#'or 'no' (no balancing of classes).
#' @example
#' all_cores <- parallel::detectCores(logical = FALSE)
#'cl <- makePSOCKcluster(all_cores)
#'registerDoParallel(cl)
#'
#' model1 <-
#' rand_forest(trees = 100, mode = "classification") %>% #this should cope with multinomial data alreadf
#'   set_engine("ranger", importance = c("impurity","impurity_corrected")) %>% #model is not tuned to increase computational speed
#'  set_mode("classification")
#'
#' yhats <- mrIMLpredicts(X= enviro_variables,Y=response_data, model1=model1, balance_data='no', model='classification',
#'tune_grid_size=5, k=10, seed = sample.int(1e8, 1)))
#'@export
mrIMLpredicts<- function(X, Y, Model, balance_data ='no', mode='regression', transformY='log',dummy=FALSE, tune_grid_size= 10, k=10, seed = sample.int(1e8, 1) ) {
n_response<- length(Y)
mod1_perf <- NULL #place to save performance matrix
internal_fit_function <- function( i ){
data <- cbind(Y[i], X) ###
colnames(data)[1] <- c('class') #define response variable for either regression or classification
if (mode=='classification'){
data$class<- as.factor(data$class)}
set.seed(seed)
data_split <- initial_split(data, prop = 0.75)
#data_splitalt <- initial_split(data, strata = class)
# extract training and testing sets
data_train <- training(data_split)
data_test <- testing(data_split)
#n fold cross validation
data_cv <- vfold_cv(data_train, v= k)
if(balance_data == 'down'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_downsample(class)
}
if(balance_data == 'up'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train) %>%
themis::step_rose(class) #ROSE works better on smaller data sets. SMOTE is an option too.
}
if(balance_data == 'no'){
data_recipe <- training(data_split) %>%
recipe(class ~., data= data_train)
}
if ( transformY == 'log'){
data_recipe %>% step_log(all_numeric(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
if ( dummy == TRUE){
data_recipe %>% step_dummy(all_nominal(), -all_outcomes()) #adds dummy variables if needed to any feature that is a factor
}
#optional recipe ingredients can be easily added to.
#step_corr(all_predictors()) %>% # removes all corrleated features
#step_center(all_predictors(), -all_outcomes()) %>% #center features
#step_scale(all_predictors(), -all_outcomes()) %>% #scale features
mod_workflow <- workflow() %>%
# add the recipe
add_recipe(data_recipe) %>%
# add the model
add_model(Model)
## Tune the model
tune_m<-tune::tune_grid(mod_workflow,
resamples = data_cv,
grid = tune_grid_size)
if (mode=='classification'){
# select the best model
best_m <- tune_m %>%
select_best("roc_auc")
}
if (mode=='regression'){
# select the best model
best_m <- tune_m %>%
select_best("rmse")
}
# final model specification
final_model <- finalize_workflow(mod_workflow,
best_m )
# now to fit the model
mod1_k <- final_model %>%
fit(data = data_train)
# make predictions and calculate deviance residuals.
if (mode=='classification'){
#predictions
yhatO <- predict(mod1_k, new_data = data_train, type='prob' )
yhat <- yhatO$.pred_1
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test, type='class' ) %>%
bind_cols(data_test %>% select(class))
resid <- devianceResids(yhatO, data_train$class)
}
if (mode=='regression'){
yhatO <- predict(mod1_k, new_data = data_train )
yhat <- yhatO$.pred
#predictions based on testing data
yhatT <- predict(mod1_k, new_data = data_test) # %>%
#bind_cols(data_test %>% select(class))
# resid <- devianceResids(yhatO, data_train$class)
resid= NULL
}
# the last fit. Useful for some functionality
last_mod_fit <-
final_model %>%
last_fit(data_split)
#save data
list(mod1_k = mod1_k, last_mod_fit=last_mod_fit,tune_m=tune_m, data=data, data_testa=data_test, data_train=data_train, yhat = yhat, yhatT = yhatT, resid = resid)
}
yhats <- future_lapply(seq(1,n_response), internal_fit_function, future.seed = TRUE)
}
#'Wrapper to estimate model-agnostic variable importance for multi-response models.
#'@param yhats A \code{list} is the list generated by mrIMLpredicts
#'@param X  A \code{dataframe} is the feature data
#'@details Calculates variable importance using based on partial dependencies but could do permutations as well (more memory intensive).
#' Key input is the object created by MrIML. Can be plotted with the plot_vi function.
#' @example
#' VI <- mrVip(yhats, X=X)
#' groupCov <- c(rep ("Host_characteristics", 1),rep("Urbanisation", 3), rep("Vegetation", 2), rep("Urbanisation",1), rep("Spatial", 2),
#' rep('Host_relatedness', 6),rep ("Host_characteristics", 1),rep("Vegetation", 2), rep("Urbanisation",1))
#' plot_vi(VI=VI,  X=X,Y=Y, modelPerf=ModelPerf, groupCov, cutoff= 0.5)
#'@export
mrVip_mutlirun <- function (X, Y, Model,
mode='regression',ity=5, transformY='log',
dummy=FALSE, tune_grid_size= 10, k=10,
seed = sample.int(1e8, 1) ) {
internal_fit_function <- function( i ){
yhats_rf <- mrIMLpredicts(X=X,
Y=Y,
Model=model_rf,
balance_data='no',
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1) ) #this will make sure dif seeds are used
VI <- mrVip(yhats_rf, X=X)
}
im <- future_lapply(seq(1,ity), internal_fit_function, future.seed = TRUE)
ImpGlobal <- as.data.frame(do.call(rbind, im)) #from cbind
}
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
library(future.apply)
cl <- parallel::makeCluster(4)
plan(cluster, workers=cl)
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
test
View(test)
yhats_rf <- mrIMLpredicts(X=X,
Y=Y,
Model=model_rf,
balance_data='no',
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
# cl <- parallel::makeCluster(4)
# plan(cluster, workers=cl)
#
# test <- mrVip_mutlirun(X=X,
#                           Y=Y,
#                           ity=4,
#                           Model=model_rf,
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1))
VI <- mrVip(yhats_rf, X=X)
#plot_vi(VI=VI,  X=X,Y=Y, modelPerf=ModelPerf_rf, cutoff= 0, mode='classification') #the cutoff reduces the number of individual models printed in the second plot.
View(VI)
View(test)
names(X)
data_vi <- test %>%
gather(names(X), key = Xvar, value = importance)
View(data_vi)
data_vi %>%
ggplot2(aes(x=Xvar, y=importance)) +
geom_boxplot()+
coord_flip()
data_vi %>%
ggplot(aes(x=Xvar, y=importance)) +
geom_boxplot()+
coord_flip()
data_vi %>%
ggplot(aes(x=reorder(Xvar,importance) y=importance)) +
data_vi %>%
ggplot(aes(x=reorder(Xvar,importance), y=importance)) +
geom_boxplot()+
coord_flip()
data_vi %>%
ggplot(aes(x=reorder(Xvar,importance), y=importance)) +
geom_boxplot()+
coord_flip()+
theme_bw()
data_vi %>%
ggplot(aes(x=reorder(Xvar,importance), y=importance)) +
geom_boxplot()+
coord_flip()+
theme_bw()+
labs(x="Features", y="Importance")
data_vi %>%
ggplot(aes(x=reorder(Xvar,importance), y=importance)) +
geom_boxplot()+
coord_flip()+
theme_bw()+
labs(x="Features", y="Importance")+
scale_fill_brewer(palette="BuPu")
data_vi %>%
ggplot(aes(x=reorder(Xvar,importance), y=importance, fill=Xvar)) +
geom_boxplot()+
coord_flip()+
theme_bw()+
labs(x="Features", y="Importance")+
scale_fill_brewer(palette="BuPu")#from cbind
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
# cl <- parallel::makeCluster(4)
# plan(cluster, workers=cl)
#
test <- mrVip_mutlirun(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
mrVip_mutlirun
#'Wrapper to estimate and plot model-agnostic variable importance with a measure of uncertainty for multi-response models.
#'@param Y A \code{dataframe} is response variable data (species, OTUs, SNPs etc).
#'@param X A \code{dataframe} represents predictor or feature data.
#'@param dummy A \code{logical} 'TRUE or FALSE'.
#'@param Model 1 A \code{list} can be any model from the tidy model package. See examples.
#'@param tune_grid_size A \code{numeric} sets the grid size for hyperparamter tuning. Larger grid sizes increase computational time.
#'@param k A \code{numeric} sets the number of folds in the 10-fold cross-validation. 10 is the default.
#'@seed A \code{numeric} as these models have a stochastic component, a seed is set to make to make the analysis reproducible. Defaults between 100 million and 1.
#'@param mode \code{character}'classification' or 'regression' i.e., is the generative model a regression or classification?
#'@details Calculates variable importance across multiple runs to quantify uncertainty in model estimates.
#'# this is particularly useful for smaller unbalanced data sets where the vip measure of variable
#'#importance can be a bit unstable.
#' @example
#' # test <- mrVip_mutlirun(X=X,
#'                           Y=Y,
#'                           ity=4, #runs the model 4 times and summarizes.
#'                          Model=model_rf,
#'                          mode='classification',
#'                           tune_grid_size=5,
#'                          seed = sample.int(1e8, 1))#'@export
mrVip_uncertainty<- function (X, Y, Model,
mode='regression',ity=5, transformY='log',
dummy=FALSE, tune_grid_size= 10, k=10,
seed = sample.int(1e8, 1) ) {
internal_fit_function <- function( i ){
yhats_rf <- mrIMLpredicts(X=X,
Y=Y,
Model=model_rf,
balance_data='no',
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1) ) #this will make sure dif seeds are used
VI <- mrVip(yhats_rf, X=X)
}
im <- future_lapply(seq(1,ity), internal_fit_function, future.seed = TRUE)
ImpGlobal <- as.data.frame(do.call(rbind, im))
#make wide to long
data_vi <- ImpGlobal %>%
gather(names(X), key = Xvar, value = importance)
#boxplot
data_vi %>%
ggplot(aes(x=reorder(Xvar,importance), y=importance, fill=Xvar)) +
geom_boxplot()+
coord_flip()+
theme_bw()+
labs(x="Features", y="Importance")+
scale_fill_brewer(palette="BuPu")#from cbind
}
# yhats_rf <- mrIMLpredicts(X=X,
#                           Y=Y,
#                           Model=model_rf,
#                           balance_data='no',
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1) ) ## in MrTidymodels. Balanced data= up upsamples and down downsampled to create a balanced set
# save the model
#save(yhats, file='rf_model')
# cl <- parallel::makeCluster(4)
# plan(cluster, workers=cl)
#
# test <- mrVip_mutlirun(X=X,
#                           Y=Y,
#                           ity=4,
#                           Model=model_rf,
#                           mode='classification',
#                           tune_grid_size=5,
#                           seed = sample.int(1e8, 1))
test <- mrVip_uncertainty(X=X,
Y=Y,
ity=4,
Model=model_rf,
mode='classification',
tune_grid_size=5,
seed = sample.int(1e8, 1))
mrVip_mutlirun
install.packages("tidymodels")
install.packages("tidymodels")
install.packages("tidymodels")
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels);
install.packages("rlang")
install.packages("rlang")
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels);
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels);
install.packages("rlang")
install.packages("rlang")
#make sure you have installed devtools previously.
#Install the most uptodate version from github
#if (!requireNamespace("devtools", quietly = TRUE))
# install.packages("devtools")
#   devtools:: install_github('nfj1380/mrIML')
library(mrIML)
#other package we need:
library(vip); library(tidymodels);
